# Projeto GELO - Experimento Q-Learning com FrozenLake üßä

## Introdu√ß√£o
Este projeto implementa um agente de aprendizado por refor√ßo (Reinforcement Learning) usando o algoritmo Q-Learning para resolver o ambiente FrozenLake-v1 do Gymnasium (fork moderno do OpenAI Gym). O agente aprende a navegar em um lago congelado, evitando buracos, para alcan√ßar o objetivo.

## Tecnologias e Bibliotecas
- **Python 3.x**
- **Principais depend√™ncias:**
  - `gymnasium`: Ambiente de simula√ß√£o (FrozenLake-v1)
  - `numpy`: Opera√ß√µes num√©ricas e Q-table
  - `matplotlib`: Visualiza√ß√£o de resultados

## Algoritmos e Conceitos Aplicados

### Q-Learning
Q-Learning √© um algoritmo de aprendizado por refor√ßo off-policy que aprende uma fun√ß√£o de valor-a√ß√£o (Q-function) atrav√©s de experi√™ncias. 

#### Componentes Principais:
1. **Q-Table**: Matriz que armazena valores Q(s,a) para cada par estado-a√ß√£o
2. **Pol√≠tica Œµ-greedy**: Balan√ßo entre explora√ß√£o e aproveitamento
3. **Atualiza√ß√£o de Bellman**: F√≥rmula central do Q-Learning

### F√≥rmulas e C√°lculos Principais

#### 1. Atualiza√ß√£o Q-Learning (Equa√ß√£o de Bellman):

Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ * max_a'(Q(s',a')) - Q(s,a)]

Onde:
- Œ± (taxa_aprendizagem): Taxa de aprendizado (0.1)
- Œ≥ (fator_Desconto): Fator de desconto para recompensas futuras (0.99)
- r: Recompensa imediata
- s: Estado atual
- a: A√ß√£o tomada
- s': Pr√≥ximo estado
- max_a'(Q(s',a')): M√°ximo valor Q poss√≠vel no pr√≥ximo estado

#### 2. Decaimento do Œµ (Epsilon):

Œµ = Œµ_min + (Œµ_max - Œµ_min) * exp(-decay_rate * episode)

- Controla o balan√ßo explora√ß√£o/aproveitamento
- Come√ßa com mais explora√ß√£o (Œµ=1.0) e gradualmente aumenta aproveitamento

## Como Executar

### Instala√ß√£o
bash
# Clone o reposit√≥rio
git clone [URL_DO_REPO]
cd projeto-gelo

# Crie e ative um ambiente virtual (recomendado)
python -m venv venv
.\venv\Scripts\activate  # Windows
source venv/bin/activate # Linux/Mac

# Instale as depend√™ncias
pip install -r requirements.txt


### Execu√ß√£o

#### Treino B√°sico
bash
python src/gelo.py


#### Op√ß√µes de Configura√ß√£o
bash
# Treino completo com visualiza√ß√£o
python src/gelo.py --total-episodes 5000 --num-test-episodes 500 --visualize-test

# Treino r√°pido para testes
python src/gelo.py --total-episodes 1000 --num-test-episodes 100 --render-delay 0.005

# Ajuda com todos os par√¢metros
python src/gelo.py --help


### Par√¢metros Principais
- `--total-episodes`: N√∫mero de epis√≥dios de treino
- `--num-test-episodes`: N√∫mero de epis√≥dios de teste
- `--visualize-test`: Ativa visualiza√ß√£o dos testes
- `--render-delay`: Controla velocidade da anima√ß√£o
- `--plot-results`: Gera gr√°ficos de desempenho
- `--log-dir`: Diret√≥rio para salvar logs/resultados

## Resultados e An√°lise

### Arquivos Gerados
O experimento gera v√°rios arquivos no diret√≥rio `logs/`:
- `treinamento_[TIMESTAMP].log`: Log detalhado do treino
- `q_table_[TIMESTAMP].npy`: Q-table final salva
- `results_[TIMESTAMP].json`: M√©tricas e configura√ß√£o
- `performance_[TIMESTAMP].png`: Gr√°ficos de desempenho

### M√©tricas Coletadas
- Taxa de sucesso (% epis√≥dios completados)
- M√©dia de passos por epis√≥dio
- Recompensa m√©dia
- Evolu√ß√£o do aprendizado (via gr√°ficos)

### Visualiza√ß√µes
O script gera dois gr√°ficos principais:
1. **Passos por Epis√≥dio**: Mostra a efici√™ncia do agente
2. **Recompensa por Epis√≥dio**: Indica o sucesso do aprendizado

## Coment√°rios Finais

### Pontos Fortes
- Implementa√ß√£o completa de Q-Learning
- Sistema robusto de logs e m√©tricas
- Visualiza√ß√µes claras do progresso
- Flexibilidade via argumentos de linha de comando

### Limita√ß√µes Conhecidas
- Ambiente discreto apenas (FrozenLake)
- Renderiza√ß√£o pode n√£o funcionar em ambientes headless
- Q-table pode ser grande para estados/a√ß√µes numerosos

### Pr√≥ximos Passos Poss√≠veis
- Implementar outros algoritmos (DQN)
- Melhorar visualiza√ß√µes em tempo real
- Comparar diferentes hiperpar√¢metros

## Autor
Projeto-Gelo
Jonas da Silva Freitas
Matr√≠cula: 01716338